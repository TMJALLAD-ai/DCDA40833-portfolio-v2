<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Evaluation | Taysir "TJ" Jallad</title>
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <header>
        <nav>
            <a href="index.html">Home</a>
            <a href="lab02.html" class="active">Lab 2: AI Evaluation</a>
            <!-- Add more lab links as the semester progresses -->
        </nav>
        <h1>AI Tool Evaluation</h1>
    </header>

    <main>
        <section>
            <h2>Introduction</h2>
            <p>For this lab, I decided to explore NotebookLM and ChatGPT. I picked NotebookLM because I have heard people talking about it as a study/educational tool but I have never actually tried it myself – I was curious about how it handles research differently from regular chatbots. ChatGPT made sense as my second tool because I have been using it for a while now, so I already had some experience to reflect on. Looking at these two together felt like a good way to compare what I know versus something completely new, and they're both text-based tools but work in pretty different ways.</p>
        </section>

        <section>
            <h2>Tool 1: NotebookLM</h2>
            <h3>Capabilities</h3>
            <p>NotebookLM was honestly interesting to explore because it's so different from what I'm used to. The main thing it does is work with sources you upload – it works well with uploading documents and asking questions about them. I. What stood out was how it actually cited which source it was pulling from when it answered my questions. It found connections between the different files that I probably wouldn't have caught just reading through them separately.</p>
            <p>That said, NotebookLM definitely has its limits. When I uploaded something with a lot of data tables and formulas, it could only really talk about the concepts in general terms – it couldn't actually work with the numbers or do any calculations.</p>

            <h3>Appropriate Use</h3>
            <p>I think NotebookLM would be really useful for research projects where you have a bunch of articles or documents to read through. It could help you spot patterns and themes across different sources. It would be helpful for DCDA work when you're pulling together research for a paper or trying to understand connections between different readings.</p>
            <p>Like most AI tools, it is not great for math-heavy work or anything where you need precise calculations. I'd say it's more of a starting point tool rather than something that can do the work for you.</p>

            <h3>Ethical Considerations</h3>
            <p>One thing I appreciated about NotebookLM is that it only works with what you give it, so it's not making stuff up from random internet sources, which is a common hallucination that I find myself running into when trying to work with other LLMs. Google says they don't train on your uploaded data, which is good for privacy. However, the user still has to put a lot of trust in the tool because it's easy to take what it says at face value without actually checking the original sources yourself.</p>

            <!-- Add screenshots or examples -->
            <!-- <figure>
                <img src="images/notebooklm-example.png" alt="NotebookLM interface showing source citations">
                <figcaption>NotebookLM interface demonstrating source-grounded responses</figcaption>
            </figure> -->
        </section>

        <section>
            <h2>Tool 2: ChatGPT</h2>
            <h3>Capabilities</h3>
            <p>I've been using ChatGPT for a while now, so I have a pretty good sense of what it can and can't do successfully. It's really versatile – I've used it for everything from explaining code I don't understand to brainstorming ideas for projects to helping me rewrite something to fit a different style. It's great at conversation and can adjust its responses based on what you tell it. When I'm stuck on a problem, I can describe what's happening and it usually gives me a few different approaches to try.</p>
            <p>The biggest issue with ChatGPT is that it confidently says things that are just wrong sometimes. It'll give you an answer that sounds totally reasonable but is actually inaccurate, and unless you already know enough to catch the mistake, you might not realize it.</p>

            <h3>Appropriate Use</h3>
            <p>ChatGPT works well for brainstorming, explaining concepts, debugging code (when you already understand what the code is trying to do), and helping organize your thoughts. I use it a lot when I'm learning something new and need a concept broken down in a different way than how it was taught. It's also useful for improving writing – like if I have a rough draft that needs to sound more professional or if I need help restructuring an argument.</p>
            <p>Where it falls apart is anything that needs to be factually accurate without verification. You can't trust it for research without checking sources yourself. It's also not great for anything requiring real creativity or original thinking – it's more of a remixing tool than a creation tool. There's a temptation to let it take the wheel completely when you hit a problem, but that's counterproductive. Using it as a tutor or collaborator works way better than using it as a solution machine.</p>

            <h3>Ethical Considerations</h3>
            <p>The ethics around ChatGPT are complicated. It was trained on massive amounts of text from the internet, which means it's absorbed biases, inaccuracies, and potentially copyrighted material. OpenAI isn't super transparent about exactly what's in the training data, which makes it hard to know what you're really getting. There's also the question of how it affects learning – if students use it to write papers without actually engaging with the material, that's a problem. On the flip side, it can make certain types of information and help more accessible to people who might not have other resources. I think the key is being honest about when and how you use it. If I use ChatGPT to help me understand something or revise my work, I should be transparent about that rather than pretending I did it all on my own.</p>

            <!-- <figure>
                <img src="images/chatgpt-example.png" alt="ChatGPT conversation example">
                <figcaption>Example ChatGPT conversation</figcaption>
            </figure> -->
        </section>

        <section>
            <h2>Broader Reflections</h2>
            <h3>Personal Use in Academic and Professional Work</h3>
            <p>Looking at both of these tools together, I think the biggest thing I've learned is that AI works best when it's helping me do something, not doing it for me. With ChatGPT, I've found that using it to explain things or suggest approaches is way more valuable than asking it to write code or essays for me. NotebookLM seems similar – it can help me find connections in research, but I still need to read the actual sources and think critically about what matters. The temptation is always there to let AI handle more than it should, but that ends up being less rewarding and I learn less. I want to keep using these tools in a way that makes me better at what I do rather than more dependent on them.</p>
            
            <h3>Impact on DCDA Skills</h3>
            <p>For DCDA specifically, I can see how AI tools could be really helpful but also potentially problematic. They could save time on tedious stuff like cleaning data or generating starter code, which would let us focus more on analysis and storytelling. But there's also a risk of skipping over the fundamentals. Like, if you're always asking ChatGPT to write your code, you might not actually learn how the code works or why certain approaches are better than others. I think what makes DCDA valuable is the human part – asking good questions, understanding context, figuring out what the data actually means rather than just what it shows. AI can find patterns, but it can't tell you which patterns are meaningful or what you should do about them.</p>
            
            <h3>Evaluating Future AI Tools</h3>
            <p>Going through this evaluation process has made me think more carefully about what questions I should be asking whenever I try a new AI tool. I want to know what it was trained on, how accurate it actually is, and whether it's designed to be transparent about its limitations. I also want to think about who benefits from the tool and who might be negatively affected by it. The AI landscape changes so fast that whatever's cutting-edge now might be outdated in a few months, so having a framework for evaluation feels more useful than just learning one specific tool. The main thing is staying critical and not just accepting AI outputs at face value – keeping that skeptical mindset while still being open to using these tools when they actually help.</p>
        </section>
    </main>

    <footer>
        <p>&copy; 2026 Taysir "TJ" Jallad | <a href="https://github.com/yourusername/dcda-portfolio">GitHub</a></p>
    </footer>
</body>
</html>